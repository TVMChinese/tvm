# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# HLearning, 2021
# Xiaoyu Zhang, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:33+0000\n"
"Last-Translator: Xiaoyu Zhang, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:4
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_optimize_operators_opt_conv_cuda.py>` to download "
"the full example code"
msgstr ""

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:13
msgid "How to optimize convolution on GPU"
msgstr "如何在GPU上优化卷积"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:14
msgid ""
"**Author**: `Haichen Shen <https://homes.cs.washington.edu/~haichen/>`_"
msgstr "**作者**: `Haichen Shen <https://homes.cs.washington.edu/~haichen/>`_"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:16
msgid ""
"In this tutorial, we will demonstrate how to write a high performance "
"convolution implementation in TVM. We use square size input tensors and "
"filters as an example, and assume the input to convolution has a large "
"batch. In this example, we use a different layout to store the data in order"
" to achieve better data locality. The buffer layout is HWCN, which stands "
"for height, width, channel, batch."
msgstr ""
"在本教程中，我们将演示如何在 TVM 中编写高性能卷积实现。 我们以方形大小的输入张量和卷积核为例，并假设卷积的输入具有大的批量。 "
"在这个例子中为了实现更好的数据局部性我们使用了一个不同的数据布局来存储数据。 缓冲区布局是 HWCN，代表高度、宽度、通道、批量。 "

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:25
msgid "Preparation and Algorithm"
msgstr "准备和算法"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:27
msgid ""
"We use the fixed size for input tensors with 256 channels and 14 x 14 "
"dimensions. The batch size is 256. Convolution filters contain 512 filters "
"of size 3 x 3.  We use stride size 1 and padding size 1 for the convolution."
" The following code defines the convolution algorithm in TVM."
msgstr ""
"我们使用拥有固定尺寸的输入张量，其通道数为256，大小为14x14。批量大小为256。卷积核的大小为3x3，通道数有512个。我们将卷积层的步幅的大小和填充大小都设为1。以下代码定义了"
" TVM 中的卷积算法。 "

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:85
msgid "Memory Hierarchy"
msgstr "内存层次结构"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:87
msgid ""
"We first specify the memory hierarchy for buffers. The figure below shows "
"the GPU memory hierarchy. One important difference from CPU memory hierarchy"
" is that GPU provides a cache buffer called shared memory, which is managed "
"by programmers. Thus how to maximize the data reuse in the shared memory is "
"critical to achieve high performance in GPU kernels."
msgstr ""
"我们首先指定缓冲区的内存层次结构。 下图显示了 GPU 内存层次结构。 与 CPU 内存层次结构的一个重要区别是 GPU 提供了一个称为共享内存的缓存缓冲区，由程序员来管理。 \n"
"因此，要实现高性能的GPU内核，最大化共享内存中的数据重用是至关重要的。"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:98
msgid ""
"In this example, we load both Apad and W into buffer AA and WW, which are "
"stored in the shared memory. These bufferes will be later shared by all "
"threads within the same thread block to compute the convolution. Each thread"
" then loads its own part from shared buffer into their local registers, AL "
"and WL. BL is a local cache of output B, which is also stored in the thread "
"local registers."
msgstr ""
"在这个例子中，我们将 Apad 和 W 加载到缓冲区 AA 和 WW 中，它们存储在共享内存中。 "
"这些缓冲区稍后将由同一线程块内的所有线程共享以计算卷积。 然后每个线程从共享缓冲区加载它自己的部分到它们的本地寄存器 AL 和 WL中。 BL 是输出 "
"B 的本地缓存，也存储在线程本地寄存器中。 "

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:126
msgid "Blocking"
msgstr "分块"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:128
msgid ""
"The following code splits the workload into thread blocks and individual "
"threads. We follow the blocking scheme in the matrix multiply. As shown in "
"the figure below, given a pixel coordinate (y, x), a thread block is "
"responsible for computing a region of block_factor x block_factor (64 x 64) "
"for output channels and batch. Due to the limit of shared memory space, we "
"only load step x block_factor (8 x 64) data from Apad and B each time to "
"buffers in the shared memory."
msgstr ""
"以下代码将矩阵乘法的整个工作负载拆分为线程块和单个线程。 我们在矩阵乘法中遵循分块方案。 "
"如下图所示，对于某个批量和通道数来说，给定一个像素坐标（y，x），一个线程块负责计算一个block_factor x block_factor（64 x"
" 64）的区域。 由于共享内存空间的限制，我们每次只从 Apad 和 B 加载 step x block_factor (8 x 64) "
"数据到共享内存中的缓冲区。 "

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:180
msgid "Virtual Thread Split"
msgstr "虚拟线程拆分"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:182
msgid ""
"We further split the workload from a thread block to individual threads. To "
"avoid *memory bank conflict*, we use virtual thread to split the area into 4"
" parts, and then tile into 8x8 grids. Therefore, shown in the figure below, "
"each thread computes 4 strided grids, where size of each grid is 4 x 4."
msgstr ""

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:215
msgid "Cooperative Fetching"
msgstr ""

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:217
msgid ""
"As mentioned before, each time step we need to transfer step x block_factor "
"data from GPU global memory to shared memory. In order to reduce the memory "
"transfer per thread, the following code lets threads in the same thread "
"block coopertively fetch dependent data from global memory."
msgstr ""

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:269
msgid "Generate CUDA Kernel"
msgstr ""

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:271
msgid ""
"Finally we use TVM to generate and compile the CUDA kernel, and evaluate the"
" latency of convolution."
msgstr ""

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:295
msgid "Out:"
msgstr "输出:"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:316
msgid ""
":download:`Download Python source code: opt_conv_cuda.py <opt_conv_cuda.py>`"
msgstr ":download:`下载 Python 源代码: opt_conv_cuda.py <opt_conv_cuda.py>`"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:322
msgid ""
":download:`Download Jupyter notebook: opt_conv_cuda.ipynb "
"<opt_conv_cuda.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: opt_conv_cuda.ipynb <opt_conv_cuda.ipynb>`"

#: ../../_staging/how_to/optimize_operators/opt_conv_cuda.rst:329
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
