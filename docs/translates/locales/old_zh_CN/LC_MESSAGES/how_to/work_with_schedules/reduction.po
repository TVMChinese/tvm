# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:4
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_work_with_schedules_reduction.py>` to download "
"the full example code"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:11
msgid "Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:12
msgid "**Author**: `Tianqi Chen <https://tqchen.github.io>`_"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:14
msgid ""
"This is an introduction material on how to do reduction in TVM. "
"Associative reduction operators like sum/max/min are typical construction"
" blocks of linear algebra operations."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:18
msgid "In this tutorial, we will demonstrate how to do reduction in TVM."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:37
msgid "Describe Sum of Rows"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:38
msgid ""
"Assume we want to compute sum of rows as our example. In numpy semantics "
"this can be written as :code:`B = numpy.sum(A, axis=1)`"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:41
msgid ""
"The following lines describe the row sum operation. To create a reduction"
" formula, we declare a reduction axis using :any:`te.reduce_axis`. "
":any:`te.reduce_axis` takes in the range of reductions. :any:`te.sum` "
"takes in the expression to be reduced as well as the reduction axis and "
"compute the sum of value over all k in the declared range."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:47
msgid "The equivalent C code is as follows:"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:75
msgid "Schedule the Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:76
msgid ""
"There are several ways to schedule a reduction. Before doing anything, "
"let us print out the IR code of default schedule."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:92
#: ../../_staging/how_to/work_with_schedules/reduction.rst:133
#: ../../_staging/how_to/work_with_schedules/reduction.rst:179
#: ../../_staging/how_to/work_with_schedules/reduction.rst:235
#: ../../_staging/how_to/work_with_schedules/reduction.rst:283
#: ../../_staging/how_to/work_with_schedules/reduction.rst:323
#: ../../_staging/how_to/work_with_schedules/reduction.rst:427
msgid "Out:"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:113
msgid ""
"You can find that the IR code is quite like the C code. The reduction "
"axis is similar to a normal axis, it can be splitted."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:116
msgid ""
"In the following code we split both the row axis of B as well axis by "
"different factors. The result is a nested reduction."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:164
msgid "If we are building a GPU kernel, we can bind the rows of B to GPU threads."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:210
msgid "Reduction Factoring and Parallelization"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:211
msgid ""
"One problem of building a reduction is that we cannot simply parallelize "
"over the reduction axis. We need to divide the computation of the "
"reduction, store the local reduction result in a temporal array before "
"doing a reduction over the temp array."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:216
msgid ""
"The rfactor primitive does such rewrite of the computation. In the "
"following schedule, the result of B is written to a temporary result "
"B.rf. The factored dimension becomes the first dimension of B.rf."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:268
msgid ""
"The scheduled operator of B also get rewritten to be sum over the first "
"axis of reduced result of B.f"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:292
msgid "Cross Thread Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:293
msgid ""
"We can now parallelize over the factored axis. Here the reduction axis of"
" B is marked to be a thread. TVM allows reduction axis to be marked as "
"thread if it is the only axis in reduction and cross thread reduction is "
"possible in the device."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:298
msgid ""
"This is indeed the case after the factoring. We can directly compute BF "
"at the reduction axis as well. The final generated kernel will divide the"
" rows by blockIdx.x and threadIdx.y columns by threadIdx.x and finally do"
" a cross thread reduction over threadIdx.x"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:380
msgid "Verify the correctness of result kernel by comparing it to numpy."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:400
msgid "Describe Convolution via 2D Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:401
msgid ""
"In TVM, we can describe convolution via 2D reduction in a simple way. "
"Here is an example for 2D convolution with filter size = [3, 3] and "
"strides = [1, 1]."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:456
msgid "Define General Commutative Reduction Operation"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:457
msgid ""
"Besides the built-in reduction operations like :any:`te.sum`, "
":any:`tvm.te.min` and :any:`tvm.te.max`, you can also define your "
"commutative reduction operation by :any:`te.comm_reducer`."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:481
msgid ""
"Sometimes we would like to perform reduction that involves multiple "
"values like :code:`argmax`, which can be done by tuple inputs. See :ref"
":`reduction-with-tuple-inputs` for more detail."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:486
msgid "Summary"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:487
msgid "This tutorial provides a walk through of reduction schedule."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:489
msgid "Describe reduction with reduce_axis."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:490
msgid "Use rfactor to factor out axis if we need parallelism."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:491
msgid "Define new reduction operation by :any:`te.comm_reducer`"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:506
msgid ":download:`Download Python source code: reduction.py <reduction.py>`"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:512
msgid ":download:`Download Jupyter notebook: reduction.ipynb <reduction.ipynb>`"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:519
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

