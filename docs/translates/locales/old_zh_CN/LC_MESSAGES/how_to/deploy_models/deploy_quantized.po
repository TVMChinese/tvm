# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# HLearning, 2021
# 孟鑫, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:32+0000\n"
"Last-Translator: 孟鑫, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:4
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_deploy_models_deploy_quantized.py>` to download "
"the full example code"
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:11
msgid "Deploy a Quantized Model on Cuda"
msgstr "在Cuda上部署一个量化模型"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:12
msgid "**Author**: `Wuwei Lin <https://github.com/vinx13>`_"
msgstr "**作者**: `Wuwei Lin <https://github.com/vinx13>`_"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:14
msgid ""
"This article is an introductory tutorial of automatic quantization with TVM."
" Automatic quantization is one of the quantization modes in TVM. More "
"details on the quantization story in TVM can be found `here "
"<https://discuss.tvm.apache.org/t/quantization-story/3920>`_. In this "
"tutorial, we will import a GluonCV pre-trained model on ImageNet to Relay, "
"quantize the Relay model and then perform the inference."
msgstr ""
"本文是使用TVM进行自动量化的介绍性教程。自动量化是TVM中的量化方式之一。关于TVM量化的更多细节可以在`这里 "
"<https://discuss.tvm.apache.org/t/quantization-story/3920>`_. "
"找到。在本教程中，我们将在ImageNet上导入GluonCV预训练模型到Relay，量化Relay模型，然后执行推理。"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:46
msgid "Prepare the Dataset"
msgstr "准备数据集"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:47
msgid ""
"We will demonstrate how to prepare the calibration dataset for quantization."
" We first download the validation set of ImageNet and pre-process the "
"dataset."
msgstr "我们将演示如何准备量化的校准数据集。我们首先下载ImageNet的验证集并对数据集进行预处理。"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:90
msgid ""
"The calibration dataset should be an iterable object. We define the "
"calibration dataset as a generator object in Python. In this tutorial, we "
"only use a few samples for calibration."
msgstr "校准的数据集应该是一个可迭代对象。我们在Python中将校准数据集定义为生成器对象。在本教程中，我们只使用一些样本进行校准。"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:118
msgid "Import the model"
msgstr "导入模型"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:119
msgid ""
"We use the Relay MxNet frontend to import a model from the Gluon model zoo."
msgstr "我们使用Relay MxNet前端从Gluon模型库导入一个模型。"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:139
msgid "Quantize the Model"
msgstr "量化模型"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:140
msgid ""
"In quantization, we need to find the scale for each weight and intermediate "
"feature map tensor of each layer."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:143
msgid ""
"For weights, the scales are directly calculated based on the value of the "
"weights. Two modes are supported: `power2` and `max`. Both modes find the "
"maximum value within the weight tensor first. In `power2` mode, the maximum "
"is rounded down to power of two. If the scales of both weights and "
"intermediate feature maps are power of two, we can leverage bit shifting for"
" multiplications. This make it computationally more efficient. In `max` "
"mode, the maximum is used as the scale. Without rounding, `max` mode might "
"have better accuracy in some cases. When the scales are not powers of two, "
"fixed point multiplications will be used."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:153
msgid ""
"For intermediate feature maps, we can find the scales with data-aware "
"quantization. Data-aware quantization takes a calibration dataset as the "
"input argument. Scales are calculated by minimizing the KL divergence "
"between distribution of activation before and after quantization. "
"Alternatively, we can also use pre-defined global scales. This saves the "
"time for calibration. But the accuracy might be impacted."
msgstr ""

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:182
msgid "Run Inference"
msgstr "运行推理"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:183
msgid "We create a Relay VM to build and execute the model."
msgstr "我们创建一个Relay VM来构建和执行模型。"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:212
msgid "Out:"
msgstr "输出:"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:234
msgid ""
":download:`Download Python source code: deploy_quantized.py "
"<deploy_quantized.py>`"
msgstr ":download:`下载 Python 源代码: deploy_quantized.py <deploy_quantized.py>`"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:240
msgid ""
":download:`Download Jupyter notebook: deploy_quantized.ipynb "
"<deploy_quantized.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: deploy_quantized.ipynb "
"<deploy_quantized.ipynb>`"

#: ../../_staging/how_to/deploy_models/deploy_quantized.rst:247
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
