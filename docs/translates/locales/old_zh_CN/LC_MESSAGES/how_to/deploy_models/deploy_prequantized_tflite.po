# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# jshmsjh, 2021
# juzi, 2021
# HLearning, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:32+0000\n"
"Last-Translator: HLearning, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:4
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_deploy_models_deploy_prequantized_tflite.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_how_to_deploy_models_deploy_prequantized_tflite.py>` "
"下载完整的样例代码"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:11
msgid "Deploy a Framework-prequantized Model with TVM - Part 3 (TFLite)"
msgstr "使用TVM部署预先量化的框架模型-第3部分（TFLite）"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:12
msgid "**Author**: `Siju Samuel <https://github.com/siju-samuel>`_"
msgstr "**作者**: `Siju Samuel <https://github.com/siju-samuel>`_"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:14
msgid ""
"Welcome to part 3 of the Deploy Framework-Prequantized Model with TVM "
"tutorial. In this part, we will start with a Quantized TFLite graph and then"
" compile and execute it via TVM."
msgstr "欢迎来到使用TVM部署预先量化的框架模型教程的第3部分。在这一部分中，我们将从一个量化的TFLite图开始，然后通过TVM编译并执行它。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:18
msgid ""
"For more details on quantizing the model using TFLite, readers are "
"encouraged to go through `Converting Quantized Models "
"<https://www.tensorflow.org/lite/convert/quantization>`_."
msgstr ""
"有关使用TFLite量化模型的更多详细信息，请读者阅读`转换量化模型 "
"<https://www.tensorflow.org/lite/convert/quantization>`_."

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:22
msgid ""
"The TFLite models can be downloaded from this `link "
"<https://www.tensorflow.org/lite/guide/hosted_models>`_."
msgstr ""
"可以从此链接下载TFLite模型`link "
"<https://www.tensorflow.org/lite/guide/hosted_models>`_."

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:25
msgid ""
"To get started, Tensorflow and TFLite package needs to be installed as "
"prerequisite."
msgstr "首先，需要先安装Tensorflow和TFLite软件包。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:33
msgid ""
"Now please check if TFLite package is installed successfully, ``python -c "
"\"import tflite\"``"
msgstr "现在请检查 TFLite 包是否安装成功，``python -c \"import tflite\"``"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:37
msgid "Necessary imports"
msgstr "必要导入"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:58
msgid "Download pretrained Quantized TFLite model"
msgstr "下载预训练量化TFLite模型"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:86
msgid "Utils for downloading and extracting zip files"
msgstr "用于下载和解压 zip 文件的工具"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:113
msgid "Load a test image"
msgstr "加载一张测试图片"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:116
msgid "Get a real image for e2e testing"
msgstr "获取e2e测试的真实图像"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:143
msgid "Load a tflite model"
msgstr "加载tflite模型"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:145
msgid "Now we can open mobilenet_v2_1.0_224.tflite"
msgstr "现在我们可以打开mobilenet_v2_1.0_224.tflite"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:169
msgid ""
"Lets run TFLite pre-quantized model inference and get the TFLite prediction."
msgstr "让我们运行TFLite预量化模型推断并获得TFLite预测。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:211
msgid ""
"Lets run TVM compiled pre-quantized model inference and get the TVM "
"prediction."
msgstr "让我们运行TVM编译的预量化模型推断并获得TVM预测。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:234
msgid "TFLite inference"
msgstr "TFLite推理"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:236
msgid "Run TFLite inference on the quantized model."
msgstr "在量化模型上运行TFLite推理。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:251
msgid "TVM compilation and inference"
msgstr "TVM编译与推理"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:253
msgid ""
"We use the TFLite-Relay parser to convert the TFLite pre-quantized graph "
"into Relay IR. Note that frontend parser call for a pre-quantized model is "
"exactly same as frontend parser call for a FP32 model. We encourage you to "
"remove the comment from print(mod) and inspect the Relay module. You will "
"see many QNN operators, like, Requantize, Quantize and QNN Conv2D."
msgstr ""
"我们使用TFLite-Relay解析器将TFLite预量化图转换为Relay "
"IR。注意，预量化模型的前端解析器调用与FP32模型的前端解析器调用完全相同。我们鼓励您从print(mod)中删除注释，并检查Relay模块。您将会看到很多QNN算子，比如Requantize,"
" Quantize和QNN Conv2D。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:273
msgid ""
"Lets now the compile the Relay module. We use the \"llvm\" target here. "
"Please replace it with the target platform that you are interested in."
msgstr "现在让我们编译Relay模块。我们在这里使用 \"llvm\" 为目标。您可以将其替换为您感兴趣的目标平台。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:289
msgid "Finally, lets call inference on the TVM compiled module."
msgstr "最后，让我们在TVM编译模块上调用推理。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:303
msgid "Accuracy comparison"
msgstr "精度比较"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:305
msgid ""
"Print the top-5 labels for MXNet and TVM inference. Checking the labels "
"because the requantize implementation is different between TFLite and Relay."
" This cause final output numbers to mismatch. So, testing accuracy via "
"labels."
msgstr ""
"打印MXNet和TVM推断的前5个标签。检查标签，因为TFLite和Relay之间的requantize实现是不同的。这将导致最终输出数字不匹配。因此，通过标签测试准确性。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:323
#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:349
msgid "Out:"
msgstr "输出:"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:333
msgid "Measure performance"
msgstr "测试性能"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:334
msgid ""
"Here we give an example of how to measure performance of TVM compiled "
"models."
msgstr "在这里我们举了一个例子来说明如何测试TVM编译模型的性能。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:362
msgid ""
"Unless the hardware has special support for fast 8 bit instructions, "
"quantized models are not expected to be any faster than FP32 models. Without"
" fast 8 bit instructions, TVM does quantized convolution in 16 bit, even if "
"the model itself is 8 bit."
msgstr ""
"除非硬件对快速8位指令有特殊支持，否则量化模型不会比FP32模型更快。没有快速的8位指令，TVM用16位进行量化卷积，即使模型本身是8位的。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:366
msgid ""
"For x86, the best performance can be achieved on CPUs with AVX512 "
"instructions set. In this case, TVM utilizes the fastest available 8 bit "
"instructions for the given target. This includes support for the VNNI 8 bit "
"dot product instruction (CascadeLake or newer). For EC2 C5.12x large "
"instance, TVM latency for this tutorial is ~2 ms."
msgstr ""
"对于x86，可以在设置AVX512指令的cpu上实现最佳性能。在这种情况下，TVM为给定的目标使用最快的8位指令。这包括对VNNI "
"8位点积指令(CascadeLake或更新版本)的支持。对于EC2 C5.12x大实例，本教程的TVM延迟约为2毫秒。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:371
msgid ""
"Intel conv2d NCHWc schedule on ARM gives better end-to-end latency compared "
"to ARM NCHW conv2d spatial pack schedule for many TFLite networks. ARM "
"winograd performance is higher but it has a high memory footprint."
msgstr ""
"对于许多TFLite网络，ARM上的Intel con2d NCHWc调度比ARM NCHW con2d空间包调度提供更好的端到端延迟。ARM "
"winograd的性能更高，但它有一个高内存占用。"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:375
msgid ""
"Moreover, the following general tips for CPU performance equally applies:"
msgstr "此外，以下关于CPU 性能的提示同样适用："

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:377
msgid ""
"Set the environment variable TVM_NUM_THREADS to the number of physical cores"
msgstr "将环境变量TVM_NUM_THREADS设置为物理核的数量"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:378
msgid ""
"Choose the best target for your hardware, such as \"llvm -mcpu=skylake-"
"avx512\" or \"llvm -mcpu=cascadelake\" (more CPUs with AVX512 would come in "
"the future)"
msgstr ""
"为你的硬件选择最好的目标，比如“llvm -mcpu=skylake-avx512”或“llvm "
"-mcpu=cascadelake”(将来会有更多带有AVX512的cpu)"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:380
msgid ""
"Perform autotuning - `Auto-tuning a convolution network for x86 CPU "
"<https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_x86.html>`_."
msgstr ""
"执行自动调优-'为x86 "
"CPU自动调优卷积网络<https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_x86.html>`_."

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:382
msgid ""
"To get best inference performance on ARM CPU, change target argument "
"according to your device and follow `Auto-tuning a convolution network for "
"ARM CPU "
"<https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_arm.html>`_."
msgstr ""
"为了在ARM CPU上获得最佳的推断性能，根据您的设备改变目标参数，并遵循`自动调优ARM "
"CPU的卷积网络<https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_arm.html>`_."

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:389
msgid "**Total running time of the script:** ( 2 minutes  7.334 seconds)"
msgstr "**脚本总运行时间:** ( 2 minutes  7.334 seconds)"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:404
msgid ""
":download:`Download Python source code: deploy_prequantized_tflite.py "
"<deploy_prequantized_tflite.py>`"
msgstr ""
":download:`下载Python源码: deploy_prequantized_tflite.py "
"<deploy_prequantized_tflite.py>`"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:410
msgid ""
":download:`Download Jupyter notebook: deploy_prequantized_tflite.ipynb "
"<deploy_prequantized_tflite.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: deploy_prequantized_tflite.ipynb "
"<deploy_prequantized_tflite.ipynb>`"

#: ../../_staging/how_to/deploy_models/deploy_prequantized_tflite.rst:417
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
