# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# HLearning, 2021
# DH Luo, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:34+0000\n"
"Last-Translator: DH Luo, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:4
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_tune_with_autotvm_tune_conv2d_cuda.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_how_to_tune_with_autotvm_tune_conv2d_cuda.py>` 获取完整示例代码"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:11
msgid "Tuning High Performance Convolution on NVIDIA GPUs"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:12
msgid "**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_"
msgstr "**作者**: `Lianmin Zheng <https://github.com/merrymercy>`_"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:14
msgid ""
"This is an advanced tutorial for writing high performance tunable template "
"for NVIDIA GPU. By running auto-tuner on this template, we can outperform "
"the vendor provided library CuDNN in many cases."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:18
msgid ""
"Note that this tutorial will not run on Windows or recent versions of macOS."
" To get it to run, you will need to wrap the body of this tutorial in a "
":code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:23
msgid "Install dependencies"
msgstr "安装依赖"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:24
msgid ""
"To use autotvm package in tvm, we need to install some extra dependencies. "
"(change \"3\" to \"2\" if you use python2):"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:31
msgid ""
"To make TVM run faster in tuning, it is recommended to use cython as FFI of "
"tvm. In the root directory of tvm, execute"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:39
msgid "Now return to python code. Import packages."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:63
msgid "Step 1:  Define the search space"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:64
msgid ""
"There are plenty of useful schedule primitives in tvm. You can also find "
"some tutorials that describe them in more details, such as (1). :ref:`opt-"
"conv-gpu` (2). `Optimizing DepthwiseConv on NVIDIA GPU "
"<https://tvm.apache.org/2017/08/22/Optimize-Deep-Learning-GPU-Operators-"
"with-TVM-A-Depthwise-Convolution-Example>`_"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:69
msgid ""
"However, their implementations are manually tuned for some special input "
"shapes. In this section, we build a large enough space to cover the "
"techniques used in these tutorials. Then we rely on the efficient auto-tuner"
" to search through this space and pick some good configurations."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:74
msgid ""
"If you are familiar with writing cuda schedule, you can find the following "
"template is very general. Actually this template can be easily modified to "
"tune other operators such as depthwise convolution and gemm. In order to "
"fully understand this template, you should be familiar with the schedule "
"primitives and auto tuning API. You can refer to the above tutorials and "
":ref:`autotvm tutorial <tutorial-autotvm-matmul-x86>`"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:81
msgid ""
"It is worth noting that the search space for a conv2d operator can be very "
"large (at the level of 10^9 for some input shapes)"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:185
msgid "Step 2:  Search through the space"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:186
msgid ""
"We pick the last layer on resnet as test case. Since our space is very "
"large, :code:`XGBoostTuner` is most suitable for our case. Here we only do "
"20 trials for demonstration. In practice, making 1000 trials usually can "
"find some good kernels for this template"
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:230
#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:310
msgid "Out:"
msgstr "输出:"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:268
msgid ""
"Finally we can inspect the best config from log file, check correctness, and"
" measure running time."
msgstr ""

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:334
msgid ""
":download:`Download Python source code: tune_conv2d_cuda.py "
"<tune_conv2d_cuda.py>`"
msgstr ":download:`Python 源码下载: tune_conv2d_cuda.py <tune_conv2d_cuda.py>`"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:340
msgid ""
":download:`Download Jupyter notebook: tune_conv2d_cuda.ipynb "
"<tune_conv2d_cuda.ipynb>`"
msgstr ""
":download:`Jupyter notebook 下载: tune_conv2d_cuda.ipynb "
"<tune_conv2d_cuda.ipynb>`"

#: ../../_staging/how_to/tune_with_autotvm/tune_conv2d_cuda.rst:347
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
