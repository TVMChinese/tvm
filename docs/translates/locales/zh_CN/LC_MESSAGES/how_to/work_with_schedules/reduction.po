# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# HLearning, 2021
# Xiaoyu Zhang, 2021
# 孟鑫, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:35+0000\n"
"Last-Translator: 孟鑫, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:4
msgid ""
"Click :ref:`here "
"<sphx_glr_download_how_to_work_with_schedules_reduction.py>` to download the"
" full example code"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:11
msgid "Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:12
msgid "**Author**: `Tianqi Chen <https://tqchen.github.io>`_"
msgstr "**作者**: `Tianqi Chen <https://tqchen.github.io>`_"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:14
msgid ""
"This is an introduction material on how to do reduction in TVM. Associative "
"reduction operators like sum/max/min are typical construction blocks of "
"linear algebra operations."
msgstr "这是一篇关于如何减少TVM的介绍材料。像sum/max/min这样的关联约化算子是线性代数运算的典型构造块。"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:18
msgid "In this tutorial, we will demonstrate how to do reduction in TVM."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:37
msgid "Describe Sum of Rows"
msgstr "描述行和"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:38
msgid ""
"Assume we want to compute sum of rows as our example. In numpy semantics "
"this can be written as :code:`B = numpy.sum(A, axis=1)`"
msgstr "假设以我们想要计算行的和作为我们的示例。在numpy语义中，这可以写成:code:`B = numpy.sum(A, axis=1)`"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:41
msgid ""
"The following lines describe the row sum operation. To create a reduction "
"formula, we declare a reduction axis using :any:`te.reduce_axis`. "
":any:`te.reduce_axis` takes in the range of reductions. :any:`te.sum` takes "
"in the expression to be reduced as well as the reduction axis and compute "
"the sum of value over all k in the declared range."
msgstr ""
"以下几行描述行和操作。为了创建缩减公式，我们使用:any:`te.reduce_axis`. "
":any:`te.reduce_axis`在缩量范围内:any:`te.sum`接受要缩减的表达式以及缩减轴，并计算声明范围内所有的k值之和。"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:47
msgid "The equivalent C code is as follows:"
msgstr "等效的C代码如下所示："

#: ../../_staging/how_to/work_with_schedules/reduction.rst:75
msgid "Schedule the Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:76
msgid ""
"There are several ways to schedule a reduction. Before doing anything, let "
"us print out the IR code of default schedule."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:92
#: ../../_staging/how_to/work_with_schedules/reduction.rst:133
#: ../../_staging/how_to/work_with_schedules/reduction.rst:179
#: ../../_staging/how_to/work_with_schedules/reduction.rst:235
#: ../../_staging/how_to/work_with_schedules/reduction.rst:283
#: ../../_staging/how_to/work_with_schedules/reduction.rst:323
#: ../../_staging/how_to/work_with_schedules/reduction.rst:427
msgid "Out:"
msgstr "输出:"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:113
msgid ""
"You can find that the IR code is quite like the C code. The reduction axis "
"is similar to a normal axis, it can be splitted."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:116
msgid ""
"In the following code we split both the row axis of B as well axis by "
"different factors. The result is a nested reduction."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:164
msgid ""
"If we are building a GPU kernel, we can bind the rows of B to GPU threads."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:210
msgid "Reduction Factoring and Parallelization"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:211
msgid ""
"One problem of building a reduction is that we cannot simply parallelize "
"over the reduction axis. We need to divide the computation of the reduction,"
" store the local reduction result in a temporal array before doing a "
"reduction over the temp array."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:216
msgid ""
"The rfactor primitive does such rewrite of the computation. In the following"
" schedule, the result of B is written to a temporary result B.rf. The "
"factored dimension becomes the first dimension of B.rf."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:268
msgid ""
"The scheduled operator of B also get rewritten to be sum over the first axis"
" of reduced result of B.f"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:292
msgid "Cross Thread Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:293
msgid ""
"We can now parallelize over the factored axis. Here the reduction axis of B "
"is marked to be a thread. TVM allows reduction axis to be marked as thread "
"if it is the only axis in reduction and cross thread reduction is possible "
"in the device."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:298
msgid ""
"This is indeed the case after the factoring. We can directly compute BF at "
"the reduction axis as well. The final generated kernel will divide the rows "
"by blockIdx.x and threadIdx.y columns by threadIdx.x and finally do a cross "
"thread reduction over threadIdx.x"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:380
msgid "Verify the correctness of result kernel by comparing it to numpy."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:400
msgid "Describe Convolution via 2D Reduction"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:401
msgid ""
"In TVM, we can describe convolution via 2D reduction in a simple way. Here "
"is an example for 2D convolution with filter size = [3, 3] and strides = [1,"
" 1]."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:456
msgid "Define General Commutative Reduction Operation"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:457
msgid ""
"Besides the built-in reduction operations like :any:`te.sum`, "
":any:`tvm.te.min` and :any:`tvm.te.max`, you can also define your "
"commutative reduction operation by :any:`te.comm_reducer`."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:481
msgid ""
"Sometimes we would like to perform reduction that involves multiple values "
"like :code:`argmax`, which can be done by tuple inputs. See :ref:`reduction-"
"with-tuple-inputs` for more detail."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:486
msgid "Summary"
msgstr "总结"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:487
msgid "This tutorial provides a walk through of reduction schedule."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:489
msgid "Describe reduction with reduce_axis."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:490
msgid "Use rfactor to factor out axis if we need parallelism."
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:491
msgid "Define new reduction operation by :any:`te.comm_reducer`"
msgstr ""

#: ../../_staging/how_to/work_with_schedules/reduction.rst:506
msgid ":download:`Download Python source code: reduction.py <reduction.py>`"
msgstr ":download:`下载Python源代码: reduction.py <reduction.py>`"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:512
msgid ""
":download:`Download Jupyter notebook: reduction.ipynb <reduction.ipynb>`"
msgstr ":download:`下载Jupyter notebook: reduction.ipynb <reduction.ipynb>`"

#: ../../_staging/how_to/work_with_schedules/reduction.rst:519
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
