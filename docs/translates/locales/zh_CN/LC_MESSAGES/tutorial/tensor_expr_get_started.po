# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# HLearning, 2021
# Xiaoyu Zhang, 2021
# 安杰 许, 2021
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.8.dev1734+gca660ba1e\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-10-12 10:06+0000\n"
"PO-Revision-Date: 2021-10-13 01:40+0000\n"
"Last-Translator: 安杰 许, 2021\n"
"Language-Team: Chinese (China) (https://www.transifex.com/TVMChinese/teams/124815/zh_CN/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: zh_CN\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:4
msgid ""
"Click :ref:`here <sphx_glr_download_tutorial_tensor_expr_get_started.py>` to"
" download the full example code"
msgstr ""

#: ../../_staging/tutorial/tensor_expr_get_started.rst:13
msgid "Working with Operators Using Tensor Expression"
msgstr "使用张量表达式来处理运算符"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:14
msgid "**Author**: `Tianqi Chen <https://tqchen.github.io>`_"
msgstr "**作者**: `Tianqi Chen <https://tqchen.github.io>`_"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:16
msgid ""
"In this tutorial we will turn our attention to how TVM works with Tensor "
"Expression (TE) to define tensor computations and apply loop optimizations. "
"TE describes tensor computations in a pure functional language (that is each"
" expression has no side effects). When viewed in context of the TVM as a "
"whole, Relay describes a computation as a set of operators, and each of "
"these operators can be represented as a TE expression where each TE "
"expression takes input tensors and produces an output tensor."
msgstr ""
"本次教程，我们会关注于TVM怎样使用张量表达式（TE）来定义张量计算和优化循环。 TE "
"用一种纯函数语言来描述张量计算（这意味着每一个表达式都没有副作用）。当在TVM整体环境下来看，Relay "
"用一系列操作符来描述一个计算过程。这一系列操作符中的每一个操作符可以都被表示为一个TE表达式，并且这每一个TE表达式都采用张量输入，然后生成一个张量输出。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:24
msgid ""
"This is an introductory tutorial to the Tensor Expression language in TVM. "
"TVM uses a domain specific tensor expression for efficient kernel "
"construction. We will demonstrate the basic workflow with two examples of "
"using the tensor expression language. The first example introduces TE and "
"scheduling with vector addition. The second expands on these concepts with a"
" step-by-step optimization of a matrix multiplication with TE. This matrix "
"multiplication example will serve as the comparative basis for future "
"tutorials covering more advanced features of TVM."
msgstr ""
"这是TVM中的张量表达式语言的介绍性教程。TVM用一种领域特定张量表达式来进行高效的内核构建。我们会用两个使用张量表达式语言的例子来演示基本的工作流程。第一个例子我们会用向量加法来介绍TE和调度。第二个例子用TE进行了一个矩阵乘法的逐步优化，来拓展了这两个概念。这个矩阵乘法的示例会被当做未来关于TVM更多高级功能的教程的比较基础。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:34
msgid "Example 1: Writing and Scheduling Vector Addition in TE for CPU"
msgstr "例 1：为CPU用TE的形式写并调度一个向量加法。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:36
msgid ""
"Let's look at an example in Python in which we will implement a TE for "
"vector addition, followed by a schedule targeted towards a CPU. We begin by "
"initializing a TVM environment."
msgstr "让我们来看一个python示例，我们会实现用一个TE来表达向量加法，然后针对CPU做一个调度。我们首先初始化TVM的环境。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:65
msgid "Describing the Vector Computation"
msgstr "描述向量计算"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:66
msgid ""
"We describe a vector addition computation. TVM adopts tensor semantics, with"
" each intermediate result represented as a multi-dimensional array. The user"
" needs to describe the computation rule that generates the tensors. We first"
" define a symbolic variable ``n`` to represent the shape. We then define two"
" placeholder Tensors, ``A`` and ``B``, with given shape ``(n,)``. We then "
"describe the result tensor ``C``, with a ``compute`` operation. The "
"``compute`` defines a computation, with the output conforming to the "
"specified tensor shape and the computation to be performed at each position "
"in the tensor defined by the lambda function. Note that while ``n`` is a "
"variable, it defines a consistent shape between the ``A``, ``B`` and ``C`` "
"tensors. Remember, no actual computation happens during this phase, as we "
"are only declaring how the computation should be done."
msgstr ""
" \n"
"我们描述一个向量加法计算。TVM采用张量语义，将每一个中间结果都表示成一个多维数组。用户需要描述生成张量的计算规则。我们首先定义一个符号变量 ``n`` 来表示shape。然后我们来定义两个占位张量 ``A`` 和 ``B``，他们具有相同的shape ``(n,)``。然后我们用一个 ``compute`` 操作来描述结果张量``C``。 ``compute`` 定义了一个计算，输出张量符合指定的形状， 并在由lambda函数定义的张量中的每个位置执行计算。请注意，虽然 ``n`` 是一个变量，但它定义了 ``A``, ``B`` 和``C`` 张量之间统一的形状。请记住，在此阶段不会有任何实际的计算发生，因为我们只是声明了这个计算该怎么被执行。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:94
msgid "Lambda Functions"
msgstr "Lambda函数"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:96
msgid ""
"The second argument to the ``te.compute`` method is the function that "
"performs the computation. In this example, we're using an anonymous "
"function, also known as a ``lambda`` function, to define the computation, in"
" this case addition on the ``i``th element of ``A`` and ``B``."
msgstr ""
" ``te.compute``方法的第二个参数是执行计算的函数。在这个例子中，我们使用一个匿名函数（也被称为lambda "
"function）来定义计算，在这里是对``A``和``B`` 的第 ``i``个元素进行加法。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:102
msgid "Create a Default Schedule for the Computation"
msgstr "为计算创建一个默认的调度"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:104
msgid ""
"While the above lines describe the computation rule, we can compute ``C`` in"
" many different ways to fit different devices. For a tensor with multiple "
"axes, you can choose which axis to iterate over first, or computations can "
"be split across different threads. TVM requires that the user to provide a "
"schedule, which is a description of how the computation should be performed."
" Scheduling operations within TE can change loop orders, split computations "
"across different threads, group blocks of data together, amongst other "
"operations. An important concept behind schedules is that they only describe"
" how the computation is performed, so different schedules for the same TE "
"will produce the same result."
msgstr ""
"虽然上文定义了计算的规则，我们可以用不同的方式来计算``C``以适应不同的设备。对于一个多轴张量来说，你可以选择先迭代哪个轴，或者可以跨不同的线程来拆分计算。TVM需要用户提供一个调度，即对计算该怎样执行的描述。TE中的调度操作可以改变循环的顺序，跨不同线程拆分计算，数据分块计算等等。调度背后的一个重要概念是它们仅仅描述计算是如何被执行的，所以对一个相同的TE来说，不同的调度也会生成相同的结果。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:115
msgid ""
"TVM allows you to create a naive schedule that will compute ``C`` in by "
"iterating in row major order."
msgstr "TVM允许你创建一个简单的调度，按行优先迭代顺序来计算 ``C``。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:137
msgid "Compile and Evaluate the Default Schedule"
msgstr "编译并评估默认调度"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:138
msgid ""
"With the TE expression and a schedule, we can produce runnable code for our "
"target language and architecture, in this case LLVM and a CPU. We provide "
"TVM with the schedule, a list of the TE expressions that are in the "
"schedule, the target and host, and the name of the function we are "
"producing. The result of the output is a type-erased function that can be "
"called directly from Python."
msgstr ""
"有了TE表达式和调度，我们就能为目标语言和架构（在本例中为LLVM和CPU）生成可运行的代码。我们向TVM提供调度，使用该调度的TE表达式列表，目标和主机，以及我们生成的函数名称。输出的结果是可以直接从Python调用的一个类型被擦除的函数。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:144
msgid ""
"In the following line, we use tvm.build to create a function. The build "
"function takes the schedule, the desired signature of the function "
"(including the inputs and outputs) as well as target language we want to "
"compile to."
msgstr "接下来这行，我们用tvm.build来创建一个函数。build函数接收调度，所需的函数签名（包括输入和输出）还有我们想要编译成的目标语言。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:160
msgid ""
"Let's run the function, and compare the output to the same computation in "
"numpy. The compiled TVM function is exposes a concise C API that can be "
"invoked from any language. We begin by creating a device, which is a device "
"(CPU in this example) that TVM can compile the schedule to. In this case the"
" device is an LLVM CPU target. We can then initialize the tensors in our "
"device and perform the custom addition operation. To verify that the "
"computation is correct, we can compare the result of the output of the c "
"tensor to the same computation performed by numpy."
msgstr ""
"让我们来运行这个函数，然后将输出与numpy中相同的计算结果进行比较。这个编译后的TVM函数被暴露为一个简洁的C "
"API，可以用任何语言来调用。我们首先创建一个TVM可以将调度编译上去的设备，在本例中是LLVM "
"CPU。然后我们可以在设备中初始化张量，再执行我们自定义的计算。为了验证计算是否正确，我们可以将C张量的输出结果跟numpy执行的相同计算的结果进行比较。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:188
msgid ""
"To get a comparison of how fast this version is compared to numpy, create a "
"helper function to run a profile of the TVM generated code."
msgstr "为了比较这种形式的计算和numpy计算的速度，创建一个辅助函数来运行TVM生成的代码生成一个profile文件。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:232
#: ../../_staging/tutorial/tensor_expr_get_started.rst:284
#: ../../_staging/tutorial/tensor_expr_get_started.rst:323
#: ../../_staging/tutorial/tensor_expr_get_started.rst:376
#: ../../_staging/tutorial/tensor_expr_get_started.rst:423
#: ../../_staging/tutorial/tensor_expr_get_started.rst:595
#: ../../_staging/tutorial/tensor_expr_get_started.rst:816
#: ../../_staging/tutorial/tensor_expr_get_started.rst:872
#: ../../_staging/tutorial/tensor_expr_get_started.rst:897
#: ../../_staging/tutorial/tensor_expr_get_started.rst:968
#: ../../_staging/tutorial/tensor_expr_get_started.rst:992
#: ../../_staging/tutorial/tensor_expr_get_started.rst:1056
#: ../../_staging/tutorial/tensor_expr_get_started.rst:1124
#: ../../_staging/tutorial/tensor_expr_get_started.rst:1217
#: ../../_staging/tutorial/tensor_expr_get_started.rst:1304
#: ../../_staging/tutorial/tensor_expr_get_started.rst:1381
#: ../../_staging/tutorial/tensor_expr_get_started.rst:1452
msgid "Out:"
msgstr "输出:"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:242
msgid "Updating the Schedule to Use Paralleism"
msgstr "更新调度来使用并行"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:244
msgid ""
"Now that we've illustrated the fundamentals of TE, let's go deeper into what"
" schedules do, and how they can be used to optimize tensor expressions for "
"different architectures. A schedule is a series of steps that are applied to"
" an expression to transform it in a number of different ways. When a "
"schedule is applied to an expression in TE, the inputs and outputs remain "
"the same, but when compiled the implementation of the expression can change."
" This tensor addition, in the default schedule, is run serially but is easy "
"to parallelize across all of the processor threads. We can apply the "
"parallel schedule operation to our computation."
msgstr ""
"现在我们已经说明了TE的基础知识，让我们更深度地了解调度的作用，以及如何使用它们来优化不同加架构的张量表达式。调度是应用于将一个表达式通过多种不同的方式进行变换的一系列步骤。当调度应用于TE中的表达式时，输入和输出保持不变，但当编译时表达式的实现可能会发生变化。在默认的调度中，这种张量加运算是串行的，但很容易跨所有处理器线程并行话。我们可以将并行调度操作应用于我们的计算。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:266
msgid ""
"The ``tvm.lower`` command will generate the Intermediate Representation (IR)"
" of the TE, with the corresponding schedule. By lowering the expression as "
"we apply different schedule operations, we can see the effect of scheduling "
"on the ordering of the computation. We use the flag ``simple_mode=True`` to "
"return a readable C-style statement."
msgstr ""
"``tvm.lower``命令将生成TE的中间表示（IR），以及对应的调度。通过lower我们应用不同的调度操作的表达式，我们可以看到调度对计算顺序的影响。我们使用``simple_mode=True``标志来返回一个可读的C风格语句。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:303
msgid ""
"It's now possible for TVM to run these blocks on independent threads. Let's "
"compile and run this new schedule with the parallel operation applied:"
msgstr "TVM现在可以在独立的线程中运行这些块。让我们编译和运行这个新的应用了并行操作的调度："

#: ../../_staging/tutorial/tensor_expr_get_started.rst:332
msgid "Updating the Schedule to Use Vectorization"
msgstr "更新调度以使用向量化"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:333
msgid ""
"Modern CPUs also have the ability to perform SIMD operations on floating "
"point values, and we can apply another schedule to our computation "
"expression to take advantage of this. Accomplishing this requires multiple "
"steps: first we have to split the schedule into inner and outer loops using "
"the split scheduling primitive. The inner loops can use vectorization to use"
" SIMD instructions using the vectorize scheduling primitive, then the outer "
"loops can be parallelized using the parallel scheduling primitive. Choose "
"the split factor to be the number of threads on your CPU."
msgstr ""
"现代CPU还能够对浮点值执行SIMD操作，我们可以通过将另外一个调度应用到我们的表达式计算中以利用这一点。实现这个需要多个步骤：首先，我们必须使用split调度原语将调度拆分为内循环和外循环。内循环可以使用矢量化调度源语来使用SIMD指令进行矢量化，外循环可以使用并行调度源语并行化。选择拆分因子为你CPU上的线程数。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:401
msgid "Comparing the Diferent Schedules"
msgstr "比较不同的调度"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:402
msgid "We can now compare the different schedules"
msgstr "我们现在可以比较不同的调度"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:435
msgid "Code Specialization"
msgstr "代码定制化"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:437
msgid ""
"As you may have noticed, the declarations of ``A``, ``B`` and ``C`` all take"
" the same shape argument, ``n``. TVM will take advantage of this to pass "
"only a single shape argument to the kernel, as you will find in the printed "
"device code. This is one form of specialization."
msgstr ""
"你可能已经注意到，``A``, ``B`` 和``C`` 的声明都采用相同的形状参数 "
"``n``。TVM将利用这一点将单个形状参数传递给内核，正如你在打印的设备代码中可以找到的那样。这是定制化的一种形式。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:442
msgid ""
"On the host side, TVM will automatically generate check code that checks the"
" constraints in the parameters. So if you pass arrays with different shapes "
"into fadd, an error will be raised."
msgstr "在主机端，TVM会自动生成检查代码，检查参数中的约束。因此，如果将具有不同形状的数组传递给fadd，则会出错。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:446
msgid ""
"We can do more specializations. For example, we can write :code:`n = "
"tvm.runtime.convert(1024)` instead of :code:`n = te.var(\"n\")`, in the "
"computation declaration. The generated function will only take vectors with "
"length 1024."
msgstr ""
"我们可以做更多的定制化。例如，我们可以在计算声明中写 :code:`n = tvm.runtime.convert(1024)` 而不是 "
":code:`n = te.var(\"n\")` 。生成的函数将只接收长度为1024的向量。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:451
msgid ""
"We've defined, scheduled, and compiled a vector addition operator, which we "
"were then able to execute on the TVM runtime. We can save the operator as a "
"library, which we can then load later using the TVM runtime."
msgstr ""
"我们已经定义，调度和编译了一个向量加法运算符，然后我们就可以在TVM运行时执行它。我们课可以将运算符保存为一个库，然后我们可以使用TVM运行时在稍后进行加载。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:456
msgid "Targeting Vector Addition for GPUs (Optional)"
msgstr "TVM可以适配多种架构。在下一个示例中，我们将针对GPU编译向量加法。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:457
msgid ""
"TVM is capable of targeting multiple architectures. In the next example, we "
"will target compilation of the vector addition to GPUs."
msgstr "TVM可以适配多种架构。在下一个例子中，我们将基于GPU编译矢量加法。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:561
msgid "Saving and Loading Compiled Modules"
msgstr "保存和加载编译好的模块"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:562
msgid ""
"Besides runtime compilation, we can save the compiled modules into a file "
"and load them back later."
msgstr "除了运行时编译之外，我们还可以将编译后的模块保存到一个文件中，稍后再加载它们。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:565
msgid "The following code first performs the following steps:"
msgstr "以下代码首先执行以下步骤： "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:567
msgid "It saves the compiled host module into an object file."
msgstr "它将编译后的host模块保存到一个目标文件中。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:568
msgid "Then it saves the device module into a ptx file."
msgstr "然后它将device模块保存到一个 ptx 文件中。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:569
msgid "cc.create_shared calls a compiler (gcc) to create a shared library"
msgstr "cc.create_shared 调用编译器（gcc）来创建共享库"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:603
msgid "Module Storage Format"
msgstr "模块存储格式"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:605
msgid ""
"The CPU (host) module is directly saved as a shared library (.so). There can"
" be multiple customized formats of the device code. In our example, the "
"device code is stored in ptx, as well as a meta data json file. They can be "
"loaded and linked separately via import."
msgstr ""
"CPU（主机）模块直接保存为共享库（.so）。 设备代码可以有多种自定义格式。 在我们的示例中，设备代码存储在 ptx 中，也是一个元数据 json "
"文件。 它们可以通过导入单独加载和链接。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:611
msgid "Load Compiled Module"
msgstr "加载编译好的模块"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:612
msgid ""
"We can load the compiled module from the file system and run the code. The "
"following code loads the host and device module separately and links them "
"together. We can verify that the newly loaded function works."
msgstr "我们可以从文件系统加载编译好的模块并运行代码。 以下代码分别加载主机和设备模块并将它们链接在一起。 我们可以验证新加载的函数是否有效。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:643
msgid "Pack Everything into One Library"
msgstr "将所有内容打包到一个库中"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:644
msgid ""
"In the above example, we store the device and host code separately. TVM also"
" supports export everything as one shared library. Under the hood, we pack "
"the device modules into binary blobs and link them together with the host "
"code. Currently we support packing of Metal, OpenCL and CUDA modules."
msgstr ""
"在上面的例子中，我们分别存储设备和主机代码。 TVM 还支持将所有内容导出为一个共享库。 在底层，我们将设备模块打包成二进制 "
"blob，并将它们与主机代码链接在一起。 目前我们支持 Metal、OpenCL 和 CUDA 模块的打包。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:664
msgid "Runtime API and Thread-Safety"
msgstr "运行时API和线程安全"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:666
msgid ""
"The compiled modules of TVM do not depend on the TVM compiler. Instead, they"
" only depend on a minimum runtime library. The TVM runtime library wraps the"
" device drivers and provides thread-safe and device agnostic calls into the "
"compiled functions."
msgstr ""
"TVM的编译模块不依赖于TVM编译器。相反，它只依赖于最小的运行时库。TVM运行时库包装了设备驱动程序，并为编译的函数提供了线程安全和设备无关的调用。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:671
msgid ""
"This means that you can call the compiled TVM functions from any thread, on "
"any GPUs, provided that you have compiled the code for that GPU."
msgstr "这意味着你可以从任何线程，任何GPU上调用已编译的TVM函数，前提是你已经为该GPU编译了代码。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:675
msgid "Generate OpenCL Code"
msgstr "产生OpenCL代码"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:676
msgid ""
"TVM provides code generation features into multiple backends. We can also "
"generate OpenCL code or LLVM code that runs on CPU backends."
msgstr "TVM 向多个后端提供代码生成功能。 我们还可以生成在 CPU 后端运行的 OpenCL 代码或 LLVM 代码。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:679
msgid ""
"The following code blocks generate OpenCL code, creates array on an OpenCL "
"device, and verifies the correctness of the code."
msgstr "以下代码块生成 OpenCL 代码，在 OpenCL 设备上创建数组，并验证代码的正确性。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:704
msgid "TE Scheduling Primitives"
msgstr "TE调度源语"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:706
msgid "TVM includes a number of different scheduling primitives:"
msgstr "TVM包括许多不同的调度源语："

#: ../../_staging/tutorial/tensor_expr_get_started.rst:708
msgid "split: splits a specified axis into two axises by the defined factor."
msgstr "split：将指定的轴按定义的因子拆分为两个轴。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:709
msgid ""
"tile: tiles will split a computation across two axes by the defined factors."
msgstr "tile：tile将按照定义的因子将计算拆分到两个轴上。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:710
msgid "fuse: fuses two consecutive axises of one computation."
msgstr "fuse：融合一个计算的两个连续轴。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:711
msgid "reorder: can reorder the axises of a computation into a defined order."
msgstr "reorder：可以将计算的轴重排为定义的顺序。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:712
msgid ""
"bind: can bind a computation to a specific thread, useful in GPU "
"programming."
msgstr "bind：可以将计算绑定到特定线程，这在 GPU 编程中很有用。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:713
msgid ""
"compute_at: by default, TVM will compute tensors at the outermost level of "
"the function, or the root, by default. compute_at specifies that one tensor "
"should be computed at the first axis of computation for another operator."
msgstr ""
"compute_at：默认情况下，TVM 将在函数的最外层或根计算张量。 compute_at 指定应该在另一个算子的第一个计算轴上计算一个张量。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:717
msgid ""
"compute_inline: when marked inline, a computation will be expanded then "
"inserted into the address where the tensor is required."
msgstr "compute_inline：当标记为内联时，计算将被扩展然后插入到被张量需要的地址中。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:719
msgid ""
"compute_root: moves a computation to the outermost layer, or root, of the "
"function. This means that stage of the computation will be fully computed "
"before it moves on to the next stage."
msgstr "compute_root：将计算移动到函数的最外层或根。 这意味着在进入下一个阶段之前，计算的阶段将被完全计算。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:723
msgid ""
"A complete description of these primitives can be found in the [Schedule "
"Primitives](https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html)"
" docs page."
msgstr ""
"这些原语的完整描述可以在 "
"[调度源语](https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html)"
" 文档页面中找到。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:727
msgid "Example 2: Manually Optimizing Matrix Multiplication with TE"
msgstr "示例 2：使用 TE 手动优化矩阵乘法"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:729
msgid ""
"Now we will consider a second, more advanced example, demonstrating how with"
" just 18 lines of python code TVM speeds up a common matrix multiplication "
"operation by 18x."
msgstr "现在我们将考虑第二个更高级的示例，演示如何仅使用 18 行 Python 代码 TVM 将常见矩阵乘法运算速度提高 18 倍。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:732
msgid ""
"**Matrix multiplication is a compute intensive operation. There are two "
"important optimizations for good CPU performance:**"
msgstr "**矩阵乘法是计算密集型操作。要实现良好的CPU性能有两个重要的优化：** "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:735
msgid ""
"Increase the cache hit rate of memory access. Both complex numerical "
"computation and hot-spot memory access can be accelerated by a high cache "
"hit rate. This requires us to transform the origin memory access pattern to "
"a pattern that fits the cache policy."
msgstr ""
"提高内存访问的缓存命中率。复杂的数值计算和热点内存访问都可以通过高缓存命中率来加速。 这需要我们将原始内存访问模式转换为适合缓存策略的模式。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:741
msgid ""
"SIMD (Single instruction multi-data), also known as the vector processing "
"unit. On each cycle instead of processing a single value, SIMD can process a"
" small batch of data.  This requires us to transform the data access pattern"
" in the loop body in uniform pattern so that the LLVM backend can lower it "
"to SIMD."
msgstr ""
"SIMD（单指令多数据），或者我们称之为向量处理单元。 每次都会处理一小批数据，而不是单个数据。 这需要我们统一变换循环体中的数据访问模式，以便 "
"LLVM 后端可以将其lower到SIMD。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:747
msgid ""
"The techniques used in this tutorial are a subset of tricks mentioned in "
"this `repository <https://github.com/flame/how-to-optimize-gemm>`_. Some of "
"them have been applied by TVM abstraction automatically, but some of them "
"cannot be automatically applied due to TVM constraints."
msgstr ""
"实际上，本教程中使用的所有方法都是这个 `repo <https://github.com/flame/how-to-optimize-gemm>`_ "
"中提到的技巧的一个子集。 其中一些已经被TVM自动抽象所采用，但由于 TVM 的限制，其中一些不能简单地应用。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:753
msgid "Preparation and Performance Baseline"
msgstr "准备和基线（Baseline）"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:755
msgid ""
"We begin by collecting performance data on the `numpy` implementation of "
"matrix multiplication."
msgstr "我们首先收集有关矩阵乘法的 `numpy` 实现的性能数据。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:824
msgid ""
"Now we write a basic matrix multiplication using TVM TE and verify that it "
"produces the same results as the numpy implementation. We also write a "
"function that will help us measure the performance of the schedule "
"optimizations."
msgstr ""
"现在我们使用 TVM TE 编写一个基本的矩阵乘法，并验证它可以产生和 numpy 实现相同的结果。 我们还编写了一个函数来帮助我们测量调度优化的性能。"
" "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:880
msgid ""
"Let's take a look at the intermediate representation of the operator and "
"default schedule using the TVM lower function. Note how the implementation "
"is essentially a naive implementation of a matrix multiplication, using "
"three nested loops over the indices of the A and B matrices."
msgstr ""
"让我们使用 TVM lowe函数看一下算子的中间表示（IR）和默认的调度。 请注意，该实现本质上是一个原始的矩阵乘法实现，在 A 和 B "
"矩阵的索引上使用三个嵌套的循环。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:922
msgid "Optimization 1: Blocking"
msgstr "优化1：分块"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:924
msgid ""
"A important trick to enhance the cache hit rate is blocking, where you "
"structure memory access such that the inside a block is a small neighborhood"
" that has high memory locality. In this tutorial, we pick a block factor of "
"32. This will result in a block that will fill a 32 * 32 * sizeof(float) "
"area of memory. This corresponds to a cache size of 4KB, in relation to a "
"reference cache size of 32 KB for L1 cache."
msgstr ""
"提高缓存命中率的一个重要技巧是分块——数据块将逐块进行计算。 块内部的内存访问是一个具有高内存局部性的小邻域。 在本教程中，我选择了 32 "
"作为分块因子。 所以该块将填充 32 * 32 * sizeof(float) 即 4KB 到缓存中，缓存的总大小为 32KB（L1 数据缓存）"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:931
msgid ""
"We begin by creating a default schedule for the ``C`` operation, then apply "
"a ``tile`` scheduling primitive to it with the specified block factor, with "
"the scheduling primitive returning the resulting loop order from outermost "
"to innermost, as a vector ``[x_outer, y_outer, x_inner, y_inner]``. We then "
"get the reduction axis for output of the operation, and perform a split "
"operation on it using a factor of 4. This factor doesn't directly impact the"
" blocking optimization we're working on right now, but will be useful later "
"when we apply vectorization."
msgstr ""
"我们首先为`C`` 运算符创建一个默认的调度，然后基于指定的块因子对其应用 ``tile`` 调度源语，该源语从外层到最内层循环的结果顺序，如向量 "
"``[x_outer, y_outer, x_inner, y_inner]`` "
"所示。然后我们获得运算符输出的约分轴，并对其使用因子为4的拆分操作。这个因子不会直接影响我们现在正在进行的分块优化，但稍后我们应用矢量化会用到。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:940
msgid ""
"Now that the operation has been blocked, we can reorder the computation to "
"put the reduction operation into the outermost loop of the computation, "
"helping to guarantee that the blocked data remains in cache. This completes "
"the schedule, and we can build and test the performance compared to the "
"naive schedule."
msgstr ""
"既然操作已经分好块了，我们可以对计算进行重排，将约分操作放到计算的最外层循环中，以保证块里面的数据都能保留在缓存中。这就完成了调度，然后我们就可以构建程序并测试与朴素调度相比的性能了。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:976
msgid ""
"By reordering the computation to take advantage of caching, you should see a"
" significant improvement in the performance of the computation. Now, print "
"the internal representation and compare it to the original:"
msgstr "通过重排计算以利用缓存，你应该会看到计算性能的显著提高。 现在，打印中间表示并将其与原始表示进行比较： "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1027
msgid "Optimization 2: Vectorization"
msgstr "优化2：向量化"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1029
msgid ""
"Another important optimization trick is vectorization. When the memory "
"access pattern is uniform, the compiler can detect this pattern and pass the"
" continuous memory to the SIMD vector processor. In TVM, we can use the "
"``vectorize`` interface to hint the compiler this pattern, taking advantage "
"of this hardware feature."
msgstr ""
"另一个重要的技巧是矢量化。 当内存访问模式一致时，编译器可以检测到这种模式并将连续内存传递给向量处理器。 在 TVM 中，我们可以使用 "
"`vectorize` 接口来提示编译器这种模式，利用硬件的特性。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1035
msgid ""
"In this tutorial, we chose to vectorize the inner loop row data since it is "
"already cache friendly from our previous optimizations."
msgstr "在本教程中，我们选择矢量化内循环行数据，因为它已经从我们之前的优化中对缓存是友好的。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1088
msgid "Optimization 3: Loop Permutation"
msgstr "优化3：循环重排"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1090
msgid ""
"If we look at the above IR, we can see the inner loop row data is vectorized"
" and B is transformed into PackedB (this is evident by the `(float32x32*)B2`"
" portion of the inner loop). The traversal of PackedB is sequential now. So "
"we will look at the access pattern of A. In current schedule, A is accessed "
"column by column which is not cache friendly. If we change the nested loop "
"order of `ki` and inner axes `xi`, the access pattern for A matrix will be "
"more cache friendly."
msgstr ""
"如果我们查看上面的 IR，我们可以看到 B 和 C 的内循环行数据都进行了向量化。接下来我们将查看 A 的访问模式。在当前调度中，A "
"是逐列访问的，这对缓存不友好。 如果我们改变  `ki` 和内轴  `xi` 的嵌套循环顺序，A 矩阵的访问模式会对缓存更加友好。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1156
msgid "Optimization 4: Array Packing"
msgstr "优化4：数组压缩"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1158
msgid ""
"Another important trick is array packing. This trick is to reorder the "
"storage dimension of the array to convert the continuous access pattern on "
"certain dimension to a sequential pattern after flattening."
msgstr "另一个重要的技巧是数组压缩。这个技巧是对多维数组的存储顺序进行重排，以便在将其展平并存储在一维内存中后按顺序访问。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1165
msgid ""
"Just as it is shown in the figure above, after blocking the computations, we"
" can observe the array access pattern of B (after flattening), which is "
"regular but discontinuous. We expect that after some transformation we can "
"get a continuous access pattern. By reordering a ``[16][16]`` array to a "
"``[16/4][16][4]`` array the access pattern of B will be sequential when "
"grabing the corresponding value from the packed array."
msgstr ""
"如上图所示，计算分块之后，我们可以观察到展平后B的数组访问模式不是连续的。我们期望通过一些变换后可以获得一个连续的访问模式。通过将一个 "
"``[16][16]`` 数组重排为 ``[16/4][16][4]`` 数组，当我们从压缩的数组中获得对应的值时，B的访问模式是连续的。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1172
msgid ""
"To accomplish this, we are going to have to start with a new default "
"schedule, taking into account the new packing of B. It's worth taking a "
"moment to comment on this: TE is a powerful and expressive language for "
"writing optimized operators, but it often requires some knowledge of the "
"underlying algorithm, data structures, and hardware target that you are "
"writing for. Later in the tutorial, we will discuss some of the options for "
"letting TVM take that burden. Regardless, let's move on with the new "
"optimized schedule."
msgstr ""

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1256
msgid "Optimization 5: Optimizing Block Writing Through Caching"
msgstr "优化 5：通过缓存优化块写入"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1258
msgid ""
"Up to this point all of our optimizations have focused on efficiently "
"accessing and computing the data from the `A` and `B` matrices to compute "
"the `C` matrix. After the blocking optimization, the operator will write "
"result to `C` block by block, and the access pattern is not sequential. We "
"can address this by using a sequential cache array, using a combination of "
"`cache_write`, `compute_at`, and `unroll`to hold the block results and write"
" to `C` when all the block results are ready."
msgstr ""
"至此，我们所有的优化都专注于高效访问和计算来自\"A\"和\"B\"矩阵的数据，以计算\"C\"矩阵。阻止优化后，算子会将结果逐块写入\"C\"块，并且访问模式不是连续的。我们可以通过使用顺序缓存阵列，使用\"cache_write\"，\"compute_at\"和\"展开\"的组合来保持块结果，并在所有块结果准备好时写入\"C\"。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1350
msgid "Optimization 6: Parallelization"
msgstr "优化 6：并行化"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1352
msgid ""
"So far, our computation is only designed to use a single core. Nearly all "
"modern processors have multiple cores, and computation can benefit from "
"running computations in parallel. The final optimization is to take "
"advantage of thread-level parallelization."
msgstr "到目前为止，我们的计算只设计为使用单个核心。几乎所有的现代处理器都有多个内核，计算可以同时运行计算。最终优化是利用线程级并行。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1427
msgid "Summary of Matrix Multiplication Example"
msgstr "矩阵乘法示例总结"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1429
msgid ""
"After applying the above simple optimizations with only 18 lines of code, "
"our generated code can begin to approach the performance of `numpy` with the"
" Math Kernel Library (MKL). Since we've been logging the performance as "
"we've been working, we can compare the results."
msgstr ""
"在仅使用 18 行代码应用上述简单优化后，我们生成的代码可以开始使用数学内核库 （MKL） "
"接近\"numpy\"的性能。由于我们一直在记录我们工作时的表现，我们可以比较结果。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1467
msgid ""
"Note that the outputs on the web page reflect the running times on a non-"
"exclusive Docker container, and should be considered unreliable. It is "
"highly encouraged to run the tutorial by yourself to observe the performance"
" gain achieved by TVM, and to carefully work through each example to "
"understand the iterative improvements that are made to the matrix "
"multiplication operation."
msgstr ""
"请注意，网页上的输出反映了非排他性 Docker 容器上的运行时间，应视为不可靠。我们非常鼓励自己运行教程，观察 TVM "
"的性能收益，并仔细研究每个示例，了解矩阵乘法操作的迭代改进。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1475
msgid "Final Notes and Summary"
msgstr "最终说明总结"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1476
msgid ""
"As mentioned earlier, how to apply optimizations using TE and scheduling "
"primitives can require some knowledge of the underlying architecture and "
"algorithms. However, TE was designed to act as a foundation for more complex"
" algorithms that can search the potential optimization. With the knowledge "
"you have from this introduction to TE, we can now begin to explore how TVM "
"can automate the schedule optimization process."
msgstr ""
"如前所述，如何使用 TE 和调度原始信息进行优化可能需要了解基础架构和算法。但是，TE "
"旨在作为更复杂的算法的基础，这些算法可以搜索潜在的优化。有了您从此介绍到 TE 的知识，我们现在可以开始探索 TVM 如何自动实现计划优化过程。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1483
msgid ""
"This tutorial provided a walkthrough of TVM Tensor Expresstion (TE) workflow"
" using a vector add and a matrix multiplication examples. The general "
"workflow is"
msgstr "本教程使用矢量添加和矩阵乘法示例提供了 TVM 张量表达 （TE） 工作流程的演练。一般工作流程是"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1487
msgid "Describe your computation via a series of operations."
msgstr "通过一系列算子描述您的计算。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1488
msgid "Describe how we want to compute use schedule primitives."
msgstr "描述我们如何使用调度原语去计算。 "

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1489
msgid "Compile to the target function we want."
msgstr "编译到我们想要的目标函数。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1490
msgid "Optionally, save the function to be loaded later."
msgstr "可选，保存稍后要加载的功能。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1492
msgid ""
"Upcoming tutorials expand on the matrix multiplication example, and show how"
" you can build generic templates of the matrix multiplication and other "
"operations with tunable parameters that allows you to automatically optimize"
" the computation for specific platforms."
msgstr "即将推出的教程扩展到矩阵乘法示例，并显示如何使用可调参数构建矩阵乘法和其他操作的通用模板，从而允许您自动优化特定平台的计算。"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1510
msgid ""
":download:`Download Python source code: tensor_expr_get_started.py "
"<tensor_expr_get_started.py>`"
msgstr ""
":download:`下载 Python 源码: tensor_expr_get_started.py "
"<tensor_expr_get_started.py>`"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1516
msgid ""
":download:`Download Jupyter notebook: tensor_expr_get_started.ipynb "
"<tensor_expr_get_started.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: tensor_expr_get_started.ipynb "
"<tensor_expr_get_started.ipynb>`"

#: ../../_staging/tutorial/tensor_expr_get_started.rst:1523
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
